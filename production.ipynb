{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook for end-to-end development of the customer churn model selected in the EDA.\n",
        "Please run the cells in order. There are parts of the script that require time for the resrouces to be spun up in azure. These are indicated in the markdown cells. Please wiat for the green notification for each particular cell before continuing onto the next one. Some of the parts such as creating of custom envrionment, pipeline run, endpoint etc will require aroundn 10 minutes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107221076
        }
      },
      "outputs": [],
      "source": [
        "# Checking to see if azureml has been inslalled properly.\n",
        "pip show azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107227062
        }
      },
      "outputs": [],
      "source": [
        "# Enter details of your AML workspace\n",
        "subscription_id = \"YOUR SUBSCRIPTION ID HERE\"\n",
        "resource_group = \"rg-churn-pred-proj\"\n",
        "workspace = \"churn-pred-proj\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107234025
        }
      },
      "outputs": [],
      "source": [
        "#Connecting to the MLClient\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating a data asset\n",
        "This is the data we have downloaded form hugging face website and will be using for model development."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## List Data Stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107239610
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "stores = ml_client.datastores.list()\n",
        "for ds_name in stores:\n",
        "    print(ds_name.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107245373
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import time\n",
        "\n",
        "my_path = \"./data/churn.csv\"\n",
        "# web_path = \"https://huggingface.co/datasets/scikit-learn/churn-prediction/blob/main/dataset.csv\"\n",
        "# set the version number of the data asset to the current UTC time\n",
        "v1 = time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())\n",
        "\n",
        "churn_data = Data(\n",
        "    name=\"telco-churn\",\n",
        "    version=v1,\n",
        "    description=\"Churning customers of a telecommunication company\",\n",
        "    path=my_path,\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    tags={\"source_type\": \"web\", \"source\": \"Hugging Face\"},\n",
        ")\n",
        "\n",
        "# create data asset\n",
        "ml_client.data.create_or_update(churn_data)\n",
        "\n",
        "print(f\"Data asset created. Name: {churn_data.name}, version: {churn_data.version}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Access Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U azureml-fsspec"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Check to see if we have the data or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107269439
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# get a handle of the data asset and print the URI\n",
        "data_asset = ml_client.data.get(name=\"telco-churn\", version=v1)\n",
        "print(f\"Data asset URI: {data_asset.path}\")\n",
        "\n",
        "# read into pandas - note that you will see 2 headers in your data frame - that is ok, for now\n",
        "\n",
        "df = pd.read_csv(data_asset.path)\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Cluster for Pipeline\n",
        "\n",
        "Once you run the cell, wiat and check under compute to see if the compute has been created successfully before proceeding. You also get a green notification too if you have the default settings on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107276323
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        # Name assigned to the compute cluster\n",
        "        name=\"cpu-cluster\",\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    \n",
        "    print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.begin_create_or_update(cpu_cluster)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline job environment creation\n",
        "Refresh the file explorer to make sure the directory is created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107280861
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating conda yaml file for in the dependencies directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.23.5\n",
        "  - pip=22.3.1\n",
        "  - scikit-learn=1.2.2\n",
        "  - pandas=1.5.3\n",
        "  - matplotlib=3.7.1\n",
        "  - imbalanced-learn=0.10.1\n",
        "  - pip:\n",
        "      - mlflow==2.2.2\n",
        "      - azureml-mlflow==1.51.0\n",
        "      - xgboost==1.7.5\n",
        "name: churn-env3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Custom Environment\n",
        "This can take around 5-10 minutes. Check the environment under the environment tab (custom environments). It changes from running to successful if everything is correct. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685107326140
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"Churn-Proj-scikit-learn\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Customer Churn pipeline\",\n",
        "    tags={\"scikit-learn\": \"1.2.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.1.1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Creating the environment takes around 5-10 minutes depending on the packages. <mark>DO NOT PROCEED BEFORE THE ENVIRONMENT IS MARKED AS SUCCESSFUL. </mark>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building Pipeline\n",
        "\n",
        "We require two components, Each component requires the python script (cells will write this to the appropriate file) and the yaml file.\n",
        "\n",
        "## Component 1: Data Prep\n",
        "This envolves missing vlaue recification, dropping some columns, encoding and scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685108008805
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "prep-data.py script\n",
        "This is the preparation component of the pipeline, some data cleaning, ecnoding and scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $script_folder/prep-data.py\n",
        "\n",
        "# import libraries\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # read data\n",
        "    print('Reading data ...')\n",
        "    df = get_data(args.input_data)\n",
        "\n",
        "    print('Cleaning data ...')\n",
        "    cleaned_data = clean_data(df)\n",
        "\n",
        "    print('Encoding data ...')\n",
        "    encoded_data = encode_data(cleaned_data)\n",
        "\n",
        "    print('Normalizing data ...')\n",
        "    normalized_data = normalize_data(encoded_data)\n",
        "\n",
        "    output_df = normalized_data.to_csv((Path(args.output_data) / \"churn_prepped.csv\"), index = False)\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--input_data\", dest='input_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--output_data\", dest='output_data',\n",
        "                        type=str)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Count the rows and print the result\n",
        "    row_count = (len(df))\n",
        "    print('Preparing {} rows of data'.format(row_count))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# function that removes useless values and imputes missing ones\n",
        "def clean_data(df):\n",
        "    # Column TotalCharges is a string, have to convert to numeric\n",
        "    df.TotalCharges = df.TotalCharges.apply([lambda x: float(x) if x!= ' ' else x]) # make float if value exists\n",
        "    mean = pd.to_numeric(df.TotalCharges, errors='coerce').mean()\n",
        "    df.TotalCharges = df.TotalCharges.apply([lambda x: mean if x == ' ' else x ]) # replace ' ' with mean of this column\n",
        "\n",
        "    # Drop useless columns (high cardinality and low correlation clumns - based on the ANOVA test done in EDA)\n",
        "    df.drop(['customerID', 'gender','PhoneService', 'MultipleLines',\n",
        "            'InternetService','StreamingTV', 'StreamingMovies'], axis = 1, inplace=True)\n",
        "    \n",
        "    return df   \n",
        "\n",
        "# Function that encodes the data\n",
        "def encode_data(df):\n",
        "    cat_cols = ['Partner','Dependents','OnlineSecurity','OnlineBackup',\n",
        "    \t        'DeviceProtection','TechSupport','PaperlessBilling',\n",
        "                'Contract', 'PaymentMethod'] #categorica columns\n",
        "\n",
        "    # Encode categorical columns\n",
        "    ord_enc = OrdinalEncoder()\n",
        "    df[cat_cols] = ord_enc.fit_transform(df[cat_cols]).copy() \n",
        "    # Mapping the target (Churn column)\n",
        "    lb = LabelEncoder()\n",
        "    df['Churn'] = lb.fit_transform(df['Churn'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# function that normalizes the data\n",
        "def normalize_data(df):\n",
        "    # Define Scaler\n",
        "    mms = MinMaxScaler() # Normalisation using min max scaler\n",
        "    df['tenure'] = mms.fit_transform(df[['tenure']])\n",
        "    df['MonthlyCharges'] = mms.fit_transform(df[['MonthlyCharges']])\n",
        "    df['TotalCharges'] = mms.fit_transform(df[['TotalCharges']])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing YAML file for prep-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prep-data.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: prep_data\n",
        "display_name: Prepare training data imbalanced sampling\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  input_data: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  output_data:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:Churn-Proj-scikit-learn:0.1.1\n",
        "command: >-\n",
        "  python prep-data.py \n",
        "  --input_data ${{inputs.input_data}}\n",
        "  --output_data ${{outputs.output_data}}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Component 2: Train and Evaluate Model\n",
        "We use mlflow to keep track of the training. We create the train-model.py script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $script_folder/train-model.py\n",
        "\n",
        "import mlflow\n",
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types.schema import Schema, ColSpec\n",
        "\n",
        "import glob\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "import logging\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "    mlflow.start_run()\n",
        "    # train model\n",
        "    model = train(args.learning_rate, args.max_depth , X_train, y_train)\n",
        "    # evaluate model\n",
        "    evaluate(model, X_test, y_test)\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(data_path):\n",
        "\n",
        "    all_files = glob.glob(data_path + \"/*.csv\")\n",
        "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "    return df\n",
        "\n",
        "# function that splits the data - uses SMOTE as data unbalanced.\n",
        "def split_data(df):\n",
        "\n",
        "    print(\"Splitting data...\")\n",
        "    oversample = SMOTE(sampling_strategy=1) # same sample size\n",
        "    f1 = df.iloc[:,:13].values\n",
        "    t1 = df.iloc[:,13].values\n",
        "    f1, t1 = oversample.fit_resample(f1, t1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(f1, t1, test_size=0.20, random_state=0)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# Function that trains the model, learning rate and max depth are nput args\n",
        "def train(learning_rate, max_depth, X_train, y_train):\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "    model = XGBClassifier(learning_rate = learning_rate, max_depth = int(max_depth), n_estimators = 1000)\n",
        "    model.fit(X_train,y_train)\n",
        "    return model\n",
        "\n",
        "# Function that evaluates the model\n",
        "def evaluate(model,X_test,y_test):\n",
        "\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' + str(auc))\n",
        "    metrics = {\n",
        "        \"Accuracy\": acc,\n",
        "        \"AUC\": auc,       \n",
        "        }\n",
        "    mlflow.log_metrics(metrics)\n",
        "    # Confusion Matrix\n",
        "    cm = ConfusionMatrixDisplay.from_predictions(y_test,y_hat, normalize = 'true', cmap = 'Blues')\n",
        "    #plt.savefig(\"Confusion-Matrix.png\") \n",
        "    mlflow.log_figure(cm.figure_, \"Confusion-Matrix.png\")\n",
        "    # plot ROC curve\n",
        "    roc = RocCurveDisplay.from_predictions(y_test,y_hat)\n",
        "    # Plot the diagonal 50% line\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # Plot the FPR and TPR achieved by our model\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    #plt.savefig(\"ROC-Curve.png\") \n",
        "    mlflow.log_figure(roc.figure_, \"ROC-Curve.png\")   \n",
        "    # Classification Report\n",
        "    print(classification_report(y_test,y_hat))\n",
        "    mlflow.log_text(classification_report(y_test,y_hat), \"clf_report.txt\")\n",
        "    input_schema = Schema(\n",
        "        [\n",
        "            ColSpec(\"integer\", \"SeniorCitizen\"),\n",
        "            ColSpec(\"double\", \"Partner\"),\n",
        "            ColSpec(\"double\", \"Dependents\"),\n",
        "            ColSpec(\"double\", \"tenure\"),\n",
        "            ColSpec(\"double\", \"OnlineSecurity\"),\n",
        "            ColSpec(\"double\", \"OnlineBackup\"),\n",
        "            ColSpec(\"double\", \"DeviceProtection\"),\n",
        "            ColSpec(\"double\", \"TechSupport\"),\n",
        "            ColSpec(\"double\", \"Contract\"),\n",
        "            ColSpec(\"double\", \"PaperlessBilling\"),\n",
        "            ColSpec(\"double\", \"PaymentMethod\"),\n",
        "            ColSpec(\"double\", \"MonthlyCharges\"),\n",
        "            ColSpec(\"double\", \"TotalCharges\"),\n",
        "        ]\n",
        "    )\n",
        "    output_schema = Schema([ColSpec(\"integer\")])\n",
        "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
        "    # Save Model\n",
        "    mlflow.xgboost.log_model(model, args.model_output, signature=signature)\n",
        "\n",
        "def parse_args():\n",
        "\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--learning_rate\", dest='learning_rate',\n",
        "                        type=float, default=0.01)\n",
        "    parser.add_argument(\"--max_depth\", dest='max_depth',\n",
        "                        type=int, default=3)\n",
        "    parser.add_argument(\"--model_output\", dest='model_output',\n",
        "                        type=str)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "    # run main function\n",
        "    main(args)\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train-model.YAML file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train-model.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_model\n",
        "display_name: Train an XGBoost classifier model\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  training_data: \n",
        "    type: uri_folder\n",
        "  learning_rate:\n",
        "    type: number\n",
        "    default: 0.01\n",
        "  max_depth:\n",
        "    type: integer\n",
        "    default: 3\n",
        "outputs:\n",
        "  model_output:\n",
        "    type: mlflow_model\n",
        "code: ./src\n",
        "environment: azureml:Churn-Proj-scikit-learn:0.1.1\n",
        "command: >-\n",
        "  python train-model.py \n",
        "  --training_data ${{inputs.training_data}} \n",
        "  --learning_rate ${{inputs.learning_rate}}\n",
        "  --max_depth ${{inputs.max_depth}}\n",
        "  --model_output ${{outputs.model_output}} "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685108061772
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "prep_data = load_component(source=parent_dir + \"./prep-data.yml\")\n",
        "train_XGBoost = load_component(source=parent_dir + \"./train-model.yml\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685108063657
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def customer_churn_classification(pipeline_job_input):\n",
        "    clean_data = prep_data(input_data=pipeline_job_input)\n",
        "    train_model = train_XGBoost(training_data=clean_data.outputs.output_data)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_transformed_data\": clean_data.outputs.output_data,\n",
        "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
        "    }\n",
        "\n",
        "pipeline_job = customer_churn_classification(Input(type=AssetTypes.URI_FILE, path= data_asset.path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685108066761
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "print(pipeline_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Update pipeline job to include compute and data store information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685108069410
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# change the output mode\n",
        "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\n",
        "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\"\n",
        "# set pipeline level compute\n",
        "pipeline_job.settings.default_compute = \"cpu-cluster\"\n",
        "# set pipeline level datastore\n",
        "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\n",
        "\n",
        "# print the pipeline job again to review the changes\n",
        "print(pipeline_job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Submit the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685108082381
        }
      },
      "outputs": [],
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"pipeline_churn\"\n",
        ")\n",
        "pipeline_job"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This takes at least 15-20 minutes to run through. <mark>DO NOT PROCEED BEFORE THE ENVIRONMENT IS MARKED AS SUCCESSFUL. </mark>\n",
        "\n",
        "Once the run is completed, you can see the logged metrics, ROC plot, Confusion Matrix, and feature importance plot. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Optional:\n",
        "\n",
        "If you want to register the componets created in the workspace for other users, please run the cell below. Otherwise disregard it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685046643662
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prep_data = ml_client.components.create_or_update(prep_data)\n",
        "train_XGBoost = ml_client.components.create_or_update(train_XGBoost)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Register Model\n",
        "\n",
        "Navigate to the model in the pieline output and copy the name, artifact path and the run id and use them to register the model. The run id and path will change for each experiment, so you have to get your own. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685111993788
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "run_id = 'dfcecd6b-47ad-4f85-a90a-e2c995e834dd'\n",
        "artifact_path = '/mnt/azureml/cr/j/ffc462fc02b74ca4bc5d97c910cdba2b/cap/data-capability/wd/model_output'\n",
        "model_name = 'model'\n",
        "\n",
        "mlflow.register_model(f\"runs:/{run_id}/{artifact_path}\", model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685112011284
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_example = ml_client.models.get(name=\"model\", version=\"1\")\n",
        "print(model_example)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define and create an endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685128899308
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
        "import datetime\n",
        "\n",
        "online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"Online endpoint for MLflow customer churn model\",\n",
        "    auth_mode=\"key\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685129026747
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This step take some time. Please wait until the notification pops up, or you can see the endpoint created successfully under the endpoints tab."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Configuring the deployment\n",
        "\n",
        "We already have our MLflow model to deploy. Given it is an MLflow model, we do not need to supply an environment. Normal models require the model and the environment registered separably. Need to be careful with instance type. I do not have access to all of them because of my tier, the current selection won't work on very large models, but is good for the demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685129832441
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Model, ManagedOnlineDeployment\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# Getting the registered MLflow model. Need to rpvide name and version. These are available under model tab.\n",
        "model = ml_client.models.get(name='model', version=2)\n",
        "\n",
        "# Configure a blue deployment\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=\"Standard_E2s_v3\",\n",
        "    instance_count=1,\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Deploying the model to the online endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685133923668
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The deployment of the model may take 10-15 minutes. <mark>PLEASE WAIT</mark> for deployment to complete before continuing. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To direct 100% of traffic to blue deployment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# blue deployment takes 100 traffic\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please wait until this update finishes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test the blue deployment with some sample data\n",
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    deployment_name=\"blue\",\n",
        "    request_file=\"sample_data.json\",\n",
        ")\n",
        "\n",
        "if response[1]=='1':\n",
        "    print(\"Customer Will Churn\")\n",
        "else:\n",
        "    print (\"Customer Will NOT Churn\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Warpping up the endpoint to not incure charges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

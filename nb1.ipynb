{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pip show azure-ai-ml"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Name: azure-ai-ml\r\nVersion: 1.5.0\r\nSummary: Microsoft Azure Machine Learning Client Library for Python\r\nHome-page: https://github.com/Azure/azure-sdk-for-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: azuresdkengsysadmins@microsoft.com\r\nLicense: MIT License\r\nLocation: /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages\r\nRequires: azure-common, azure-core, azure-mgmt-core, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, msrest, opencensus-ext-azure, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\r\nRequired-by: \r\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1685022817966
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enter details of your AML workspace\n",
        "subscription_id = \"733ee2ff-5a9c-42a2-8242-b8a500eb9f53\"\n",
        "resource_group = \"rg-churn-pred-proj\"\n",
        "workspace = \"churn-pred-proj\""
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1685022820491
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1685022838281
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Creating a Data Asset\n",
        "This is the data we have downloaded form hugging face website and will be using for model development."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List Data Stores"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stores = ml_client.datastores.list()\r\n",
        "for ds_name in stores:\r\n",
        "    print(ds_name.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "workspaceartifactstore\nworkspacefilestore\nworkspaceworkingdirectory\nworkspaceblobstore\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685022847184
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import time\n",
        "\n",
        "my_path = \"./data/churn.csv\"\n",
        "# web_path = \"https://huggingface.co/datasets/scikit-learn/churn-prediction/blob/main/dataset.csv\"\n",
        "# set the version number of the data asset to the current UTC time\n",
        "v1 = time.strftime(\"%Y.%m.%d.%H%M%S\", time.gmtime())\n",
        "\n",
        "churn_data = Data(\n",
        "    name=\"telco-churn\",\n",
        "    version=v1,\n",
        "    description=\"Churning customers of a telecommunication company\",\n",
        "    path=my_path,\n",
        "    type=AssetTypes.URI_FILE,\n",
        "    tags={\"source_type\": \"web\", \"source\": \"Hugging Face\"},\n",
        ")\n",
        "\n",
        "# create data asset\n",
        "ml_client.data.create_or_update(churn_data)\n",
        "\n",
        "print(f\"Data asset created. Name: {churn_data.name}, version: {churn_data.version}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading churn.csv\u001b[32m (< 1 MB): 100%|██████████| 970k/970k [00:00<00:00, 23.0MB/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data asset created. Name: telco-churn, version: 2023.05.25.135410\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1685022850305
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Access Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U azureml-fsspec"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check to see if we have the data or not."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# get a handle of the data asset and print the URI\n",
        "data_asset = ml_client.data.get(name=\"telco-churn\", version=v1)\n",
        "print(f\"Data asset URI: {data_asset.path}\")\n",
        "\n",
        "# read into pandas - note that you will see 2 headers in your data frame - that is ok, for now\n",
        "\n",
        "df = pd.read_csv(data_asset.path)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data asset URI: azureml://subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourcegroups/rg-churn-pred-proj/workspaces/churn-pred-proj/datastores/workspaceblobstore/paths/LocalUpload/980708968c362ca31f4225d8576b9cab/churn.csv\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService   \n0  7590-VHVEG  Female              0     Yes         No       1           No  \\\n1  5575-GNVDE    Male              0      No         No      34          Yes   \n2  3668-QPYBK    Male              0      No         No       2          Yes   \n3  7795-CFOCW    Male              0      No         No      45           No   \n4  9237-HQITU  Female              0      No         No       2          Yes   \n\n      MultipleLines InternetService OnlineSecurity  ... DeviceProtection   \n0  No phone service             DSL             No  ...               No  \\\n1                No             DSL            Yes  ...              Yes   \n2                No             DSL            Yes  ...               No   \n3  No phone service             DSL            Yes  ...              Yes   \n4                No     Fiber optic             No  ...               No   \n\n  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling   \n0          No          No              No  Month-to-month              Yes  \\\n1          No          No              No        One year               No   \n2          No          No              No  Month-to-month              Yes   \n3         Yes          No              No        One year               No   \n4          No          No              No  Month-to-month              Yes   \n\n               PaymentMethod MonthlyCharges  TotalCharges Churn  \n0           Electronic check          29.85         29.85    No  \n1               Mailed check          56.95        1889.5    No  \n2               Mailed check          53.85        108.15   Yes  \n3  Bank transfer (automatic)          42.30       1840.75    No  \n4           Electronic check          70.70        151.65   Yes  \n\n[5 rows x 21 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customerID</th>\n      <th>gender</th>\n      <th>SeniorCitizen</th>\n      <th>Partner</th>\n      <th>Dependents</th>\n      <th>tenure</th>\n      <th>PhoneService</th>\n      <th>MultipleLines</th>\n      <th>InternetService</th>\n      <th>OnlineSecurity</th>\n      <th>...</th>\n      <th>DeviceProtection</th>\n      <th>TechSupport</th>\n      <th>StreamingTV</th>\n      <th>StreamingMovies</th>\n      <th>Contract</th>\n      <th>PaperlessBilling</th>\n      <th>PaymentMethod</th>\n      <th>MonthlyCharges</th>\n      <th>TotalCharges</th>\n      <th>Churn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7590-VHVEG</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>1</td>\n      <td>No</td>\n      <td>No phone service</td>\n      <td>DSL</td>\n      <td>No</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Month-to-month</td>\n      <td>Yes</td>\n      <td>Electronic check</td>\n      <td>29.85</td>\n      <td>29.85</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5575-GNVDE</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>34</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>DSL</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>One year</td>\n      <td>No</td>\n      <td>Mailed check</td>\n      <td>56.95</td>\n      <td>1889.5</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3668-QPYBK</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>DSL</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Month-to-month</td>\n      <td>Yes</td>\n      <td>Mailed check</td>\n      <td>53.85</td>\n      <td>108.15</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7795-CFOCW</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>45</td>\n      <td>No</td>\n      <td>No phone service</td>\n      <td>DSL</td>\n      <td>Yes</td>\n      <td>...</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>No</td>\n      <td>One year</td>\n      <td>No</td>\n      <td>Bank transfer (automatic)</td>\n      <td>42.30</td>\n      <td>1840.75</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9237-HQITU</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>2</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Fiber optic</td>\n      <td>No</td>\n      <td>...</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Month-to-month</td>\n      <td>Yes</td>\n      <td>Electronic check</td>\n      <td>70.70</td>\n      <td>151.65</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1685022871612
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Compute Cluster for Pipeline\r\n",
        "\r\n",
        "Once you run the cell, wiat and check under compute to see if the compute has been created successfully before proceeding. You also get a green notification too if you have the default settings on. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        # Name assigned to the compute cluster\n",
        "        name=\"cpu-cluster\",\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_DS3_V2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "    \n",
        "    print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.begin_create_or_update(cpu_cluster)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Creating a new cpu compute target...\nAMLCompute with name cpu-cluster is created, the compute size is STANDARD_DS3_V2\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1685022887278
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Pipeline job environment creation\r\n",
        "Refresh the file explorer to make sure the directory is created"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dependencies_dir = \"./dependencies\"\n",
        "os.makedirs(dependencies_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1685023008475
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Creating conda yaml file for in the dependencies directory\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {dependencies_dir}/conda.yaml\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - numpy=1.23.5\n",
        "  - pip=22.3.1\n",
        "  - scikit-learn=1.2.2\n",
        "  - pandas=1.5.3\n",
        "  - matplotlib=3.7.1\n",
        "  - imbalanced-learn=0.10.1\n",
        "  - pip:\n",
        "      - mlflow==1.26.1\n",
        "      - azureml-mlflow==1.42.0\n",
        "      - xgboost==1.7.5\n",
        "name: churn-env"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./dependencies/conda.yaml\n"
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Create Environment\r\n",
        "This can take around 5-10 minutes. Check the environment under the environment tab (custom environments). It changes from running to successful if everything is correct. \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "custom_env_name = \"Churn-Proj-scikit-learn\"\n",
        "\n",
        "pipeline_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Customer Churn pipeline\",\n",
        "    tags={\"scikit-learn\": \"1.2.2\"},\n",
        "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    version=\"0.1.1\",\n",
        ")\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment with name Churn-Proj-scikit-learn is registered to workspace, the environment version is 0.1.1\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1685023063513
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the environment takes around 5-10 minutes depending on the packages. <mark>DO NOT PROCEED BEFORE THE ENVIRONMENT IS MARKED AS SUCCESSFUL. </mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# Building Pipeline\n",
        "\n",
        "We require two components, Each component requires the python script (cells will write this to the appropriate file) and the yaml file.\n",
        "\n",
        "## Component 1: Data Prep\n",
        "This envolves missing vlaue recification, dropping some columns, encoding and scaling"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "src folder created\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1685024576467
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "prep-data.py script\n",
        "This is the preparation component of the pipeline, some data cleaning, ecnoding and scaling"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/prep-data.py\n",
        "\n",
        "# import libraries\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # read data\n",
        "    print('Reading data ...')\n",
        "    df = get_data(args.input_data)\n",
        "\n",
        "    print('Cleaning data ...')\n",
        "    cleaned_data = clean_data(df)\n",
        "\n",
        "    print('Encoding data ...')\n",
        "    encoded_data = encode_data(cleaned_data)\n",
        "\n",
        "    print('Normalizing data ...')\n",
        "    normalized_data = normalize_data(encoded_data)\n",
        "\n",
        "    output_df = normalized_data.to_csv((Path(args.output_data) / \"churn_prepped.csv\"), index = False)\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--input_data\", dest='input_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--output_data\", dest='output_data',\n",
        "                        type=str)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Count the rows and print the result\n",
        "    row_count = (len(df))\n",
        "    print('Preparing {} rows of data'.format(row_count))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# function that removes useless values and imputes missing ones\n",
        "def clean_data(df):\n",
        "    # Column TotalCharges is a string, have to convert to numeric\n",
        "    df.TotalCharges = df.TotalCharges.apply([lambda x: float(x) if x!= ' ' else x]) # make float if value exists\n",
        "    mean = pd.to_numeric(df.TotalCharges, errors='coerce').mean()\n",
        "    df.TotalCharges = df.TotalCharges.apply([lambda x: mean if x == ' ' else x ]) # replace ' ' with mean of this column\n",
        "\n",
        "    # Drop useless columns (high cardinality and low correlation clumns - based on the ANOVA test done in EDA)\n",
        "    df.drop(['customerID', 'gender','PhoneService', 'MultipleLines',\n",
        "            'InternetService','StreamingTV', 'StreamingMovies'], axis = 1, inplace=True)\n",
        "    \n",
        "    return df   \n",
        "\n",
        "# Function that encodes the data\n",
        "def encode_data(df):\n",
        "    cat_cols = ['Partner','Dependents','OnlineSecurity','OnlineBackup',\n",
        "    \t        'DeviceProtection','TechSupport','PaperlessBilling',\n",
        "                'Contract', 'PaymentMethod'] #categorica columns\n",
        "\n",
        "    # Encode categorical columns\n",
        "    ord_enc = OrdinalEncoder()\n",
        "    df[cat_cols] = ord_enc.fit_transform(df[cat_cols]).copy() \n",
        "    # Mapping the target (Churn column)\n",
        "    lb = LabelEncoder()\n",
        "    df['Churn'] = lb.fit_transform(df['Churn'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# function that normalizes the data\n",
        "def normalize_data(df):\n",
        "    # Define Scaler\n",
        "    mms = MinMaxScaler() # Normalisation using min max scaler\n",
        "    df['tenure'] = mms.fit_transform(df[['tenure']])\n",
        "    df['MonthlyCharges'] = mms.fit_transform(df[['MonthlyCharges']])\n",
        "    df['TotalCharges'] = mms.fit_transform(df[['TotalCharges']])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing src/prep-data.py\n"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Preparing YAML file for prep-data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prep-data.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: prep_data\n",
        "display_name: Prepare training data imbalanced sampling\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  input_data: \n",
        "    type: uri_file\n",
        "outputs:\n",
        "  output_data:\n",
        "    type: uri_folder\n",
        "code: ./src\n",
        "environment: azureml:Churn-Proj-scikit-learn:0.1.1\n",
        "command: >-\n",
        "  python prep-data.py \n",
        "  --input_data ${{inputs.input_data}}\n",
        "  --output_data ${{outputs.output_data}}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting prep-data.yml\n"
        }
      ],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Component 2: Train and Evaluate Model\n",
        "We use mlflow to keep track of the training"
      ],
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "train-model.py script"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $script_folder/train-model.py\n",
        "\n",
        "import mlflow\n",
        "import glob\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def main(args):\n",
        "    # enable autologging\n",
        "    mlflow.autolog()\n",
        "\n",
        "    # read data\n",
        "    df = get_data(args.training_data)\n",
        "\n",
        "    # split data\n",
        "    X_train, X_test, y_train, y_test = split_data(df)\n",
        "\n",
        "    # train model\n",
        "    model = train(args.learning_rate, args.max_depth , X_train, y_train)\n",
        "\n",
        "    # evaluate model\n",
        "    evaluate(model, X_test, y_test)\n",
        "\n",
        "# function that reads the data\n",
        "def get_data(data_path):\n",
        "\n",
        "    all_files = glob.glob(data_path + \"/*.csv\")\n",
        "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# function that splits the data - uses SMOTE as data unbalanced.\n",
        "def split_data(df):\n",
        "    print(\"Splitting data...\")\n",
        "\n",
        "    oversample = SMOTE(sampling_strategy=1) # same sample size\n",
        "    f1 = df.iloc[:,:13].values\n",
        "    t1 = df.iloc[:,13].values\n",
        "    f1, t1 = oversample.fit_resample(f1, t1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(f1, t1, test_size=0.20, random_state=0)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "# Function that trains the model, learning rate and max depth are nput args\n",
        "def train(learning_rate, max_depth, X_train, y_train):\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"max_depth\", max_depth)\n",
        "\n",
        "    model = XGBClassifier(learning_rate = learning_rate, max_depth = int(max_depth), n_estimators = 1000)\n",
        "\n",
        "    model.fit(X_train,y_train)\n",
        "\n",
        "    mlflow.xgboost.save_model(model, args.model_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Function that evaluates the model\n",
        "def evaluate(model,X_test,y_test):\n",
        "    # calculate accuracy\n",
        "    y_hat = model.predict(X_test)\n",
        "    acc = np.average(y_hat == y_test)\n",
        "    print('Accuracy:', acc)\n",
        "\n",
        "    # calculate AUC\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "    print('AUC: ' + str(auc))\n",
        "\n",
        "    params = {\n",
        "        \"Accuracy\": acc,\n",
        "        \"AUC\": auc,       \n",
        "        }\n",
        "\n",
        "    mlflow.log_metrics(params)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = ConfusionMatrixDisplay.from_predictions(y_test,y_hat, normalize = 'true', cmap = 'Blues')\n",
        "    #plt.savefig(\"Confusion-Matrix.png\") \n",
        "    mlflow.log_figure(cm.figure_, \"Confusion-Matrix.png\")\n",
        "\n",
        "    # plot ROC curve\n",
        "    roc = RocCurveDisplay.from_predictions(y_test,y_hat)\n",
        "    # Plot the diagonal 50% line\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    # Plot the FPR and TPR achieved by our model\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    #plt.savefig(\"ROC-Curve.png\") \n",
        "    mlflow.log_figure(roc.figure_, \"ROC-Curve.png\")   \n",
        "\n",
        "    # Classification Report\n",
        "    print(classification_report(y_test,y_hat))\n",
        "\n",
        "    mlflow.log_text(classification_report(y_test,y_hat), \"clf_report.txt\")\n",
        "\n",
        "def parse_args():\n",
        "    # setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # add arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',\n",
        "                        type=str)\n",
        "    parser.add_argument(\"--learning_rate\", dest='learning_rate',\n",
        "                        type=float, default=0.01)\n",
        "    parser.add_argument(\"--max_depth\", dest='max_depth',\n",
        "                        type=int, default=3)\n",
        "    parser.add_argument(\"--model_output\", dest='model_output',\n",
        "                        type=str)\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "# run script\n",
        "if __name__ == \"__main__\":\n",
        "    # add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/train-model.py\n"
        }
      ],
      "execution_count": 56,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "train-model.YAML file "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train-model.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: train_model\n",
        "display_name: Train an XGBoost classifier model\n",
        "version: 1\n",
        "type: command\n",
        "inputs:\n",
        "  training_data: \n",
        "    type: uri_folder\n",
        "  learning_rate:\n",
        "    type: number\n",
        "    default: 0.01\n",
        "  max_depth:\n",
        "    type: integer\n",
        "    default: 3\n",
        "outputs:\n",
        "  model_output:\n",
        "    type: mlflow_model\n",
        "code: ./src\n",
        "environment: azureml:Churn-Proj-scikit-learn:0.1.1\n",
        "command: >-\n",
        "  python train-model.py \n",
        "  --training_data ${{inputs.training_data}} \n",
        "  --learning_rate ${{inputs.learning_rate}}\n",
        "  --max_depth ${{inputs.max_depth}}\n",
        "  --model_output ${{outputs.model_output}} "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting train-model.yml\n"
        }
      ],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Load Components"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import load_component\n",
        "parent_dir = \"\"\n",
        "\n",
        "prep_data = load_component(source=parent_dir + \"./prep-data.yml\")\n",
        "train_XGBoost = load_component(source=parent_dir + \"./train-model.yml\")"
      ],
      "outputs": [],
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1685040105525
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Build Pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "\n",
        "@pipeline()\n",
        "def customer_churn_classification(pipeline_job_input):\n",
        "    clean_data = prep_data(input_data=pipeline_job_input)\n",
        "    train_model = train_XGBoost(training_data=clean_data.outputs.output_data)\n",
        "\n",
        "    return {\n",
        "        \"pipeline_job_transformed_data\": clean_data.outputs.output_data,\n",
        "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
        "    }\n",
        "\n",
        "pipeline_job = customer_churn_classification(Input(type=AssetTypes.URI_FILE, path= data_asset.path))"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {
        "gather": {
          "logged": 1685040107071
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(pipeline_job)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "display_name: customer_churn_classification\ntype: pipeline\ninputs:\n  pipeline_job_input:\n    type: uri_file\n    path: azureml://subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourcegroups/rg-churn-pred-proj/workspaces/churn-pred-proj/datastores/workspaceblobstore/paths/LocalUpload/980708968c362ca31f4225d8576b9cab/churn.csv\noutputs:\n  pipeline_job_transformed_data:\n    type: uri_folder\n  pipeline_job_trained_model:\n    type: mlflow_model\njobs:\n  clean_data:\n    type: command\n    inputs:\n      input_data:\n        path: ${{parent.inputs.pipeline_job_input}}\n    outputs:\n      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n    component:\n      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n      name: prep_data\n      version: '1'\n      display_name: Prepare training data imbalanced sampling\n      type: command\n      inputs:\n        input_data:\n          type: uri_file\n      outputs:\n        output_data:\n          type: uri_folder\n      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n        ${{outputs.output_data}}\n      environment: azureml:Chrun-Proj-scikit-learn:3\n      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn/src\n      is_deterministic: true\n  train_model:\n    type: command\n    inputs:\n      training_data:\n        path: ${{parent.jobs.clean_data.outputs.output_data}}\n    outputs:\n      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n    component:\n      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n      name: train_model\n      version: '1'\n      display_name: Train an XGBoost classifier model\n      type: command\n      inputs:\n        training_data:\n          type: uri_folder\n        learning_rate:\n          type: number\n          default: '0.01'\n        max_depth:\n          type: integer\n          default: '3'\n      outputs:\n        model_output:\n          type: mlflow_model\n      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --learning_rate\n        ${{inputs.learning_rate}} --max_depth ${{inputs.max_depth}} --model_output\n        ${{outputs.model_output}} '\n      environment: azureml:Chrun-Proj-scikit-learn:3\n      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn/src\n      is_deterministic: true\n\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1685024628362
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update pipeline job. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the output mode\r\n",
        "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\r\n",
        "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\"\r\n",
        "# set pipeline level compute\r\n",
        "pipeline_job.settings.default_compute = \"cpu-cluster\"\r\n",
        "# set pipeline level datastore\r\n",
        "pipeline_job.settings.default_datastore = \"workspaceblobstore\"\r\n",
        "\r\n",
        "# print the pipeline job again to review the changes\r\n",
        "print(pipeline_job)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "display_name: customer_churn_classification\ntype: pipeline\ninputs:\n  pipeline_job_input:\n    type: uri_file\n    path: azureml://subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourcegroups/rg-churn-pred-proj/workspaces/churn-pred-proj/datastores/workspaceblobstore/paths/LocalUpload/980708968c362ca31f4225d8576b9cab/churn.csv\noutputs:\n  pipeline_job_transformed_data:\n    mode: upload\n  pipeline_job_trained_model:\n    mode: upload\njobs:\n  clean_data:\n    type: command\n    inputs:\n      input_data:\n        path: ${{parent.inputs.pipeline_job_input}}\n    outputs:\n      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n    component:\n      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n      name: prep_data\n      version: '1'\n      display_name: Prepare training data imbalanced sampling\n      type: command\n      inputs:\n        input_data:\n          type: uri_file\n      outputs:\n        output_data:\n          type: uri_folder\n      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n        ${{outputs.output_data}}\n      environment: azureml:Churn-Proj-scikit-learn:0.1.1\n      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn/src\n      is_deterministic: true\n  train_model:\n    type: command\n    inputs:\n      training_data:\n        path: ${{parent.jobs.clean_data.outputs.output_data}}\n    outputs:\n      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n    component:\n      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n      name: train_model\n      version: '1'\n      display_name: Train an XGBoost classifier model\n      type: command\n      inputs:\n        training_data:\n          type: uri_folder\n        learning_rate:\n          type: number\n          default: '0.01'\n        max_depth:\n          type: integer\n          default: '3'\n      outputs:\n        model_output:\n          type: mlflow_model\n      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --learning_rate\n        ${{inputs.learning_rate}} --max_depth ${{inputs.max_depth}} --model_output\n        ${{outputs.model_output}} '\n      environment: azureml:Churn-Proj-scikit-learn:0.1.1\n      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn/src\n      is_deterministic: true\nsettings:\n  default_datastore: azureml:workspaceblobstore\n  default_compute: azureml:cpu-cluster\n\n"
        }
      ],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685040110414
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Submit the pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# submit job to workspace\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline_job, experiment_name=\"pipeline_churn\"\n",
        ")\n",
        "pipeline_job"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading src (0.01 MBs):   0%|          | 0/7470 [00:00<?, ?it/s]\r\u001b[32mUploading src (0.01 MBs): 100%|██████████| 7470/7470 [00:00<00:00, 198335.47it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 60,
          "data": {
            "text/plain": "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f7834a7a200>}, 'outputs': {'pipeline_job_transformed_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f7834a78fd0>, 'pipeline_job_trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f7834a78250>}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f7834a7bbb0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'customer_churn_classification', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_transformed_data': {}, 'pipeline_job_trained_model': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'clean_data': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'clean_data', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f7834a7a9e0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_data': '${{parent.outputs.pipeline_job_transformed_data}}'}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f7834a7ac20>}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f7834a79ba0>}, 'component': 'azureml_anonymous:e3d660b4-9a4a-45c2-a158-918b1c279379', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'b1ef0c23-4fff-4553-b8f6-9b3af4edbabd', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'train_model': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'train_model', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f7834a7bac0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'training_data': '${{parent.jobs.clean_data.outputs.output_data}}'}, 'job_outputs': {'model_output': '${{parent.outputs.pipeline_job_trained_model}}'}, 'inputs': {'training_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f7834a79f60>}, 'outputs': {'model_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f7834a785b0>}, 'component': 'azureml_anonymous:c7882111-36b7-4c76-8cb9-2fd359f9b614', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'd39d49d9-5b15-4b86-9823-56595614389c', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'magenta_candle_hh9kzszdmw', 'description': None, 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'cpu-cluster', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourceGroups/rg-churn-pred-proj/providers/Microsoft.MachineLearningServices/workspaces/churn-pred-proj/jobs/magenta_candle_hh9kzszdmw', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f7834a78070>, 'serialize': <msrest.serialization.Serializer object at 0x7f7834a7be20>, 'display_name': 'customer_churn_classification', 'experiment_name': 'pipeline_churn', 'compute': None, 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f7834a78490>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x7f7834a792a0>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_churn</td><td>magenta_candle_hh9kzszdmw</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/magenta_candle_hh9kzszdmw?wsid=/subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourcegroups/rg-churn-pred-proj/workspaces/churn-pred-proj&amp;tid=8544ec55-6367-4b6b-aa13-8537941ef1af\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 60,
      "metadata": {
        "gather": {
          "logged": 1685040119170
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This takes at least 15-20 minutes to run through.\r\n",
        "Once the run is completed, you can see the logged metrics, ROC plot, Confusion Matrix, and feature importance plot. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional:\r\n",
        "\r\n",
        "If you want to register the componets created in the workspace for other users, please run the cell below. Otherwise disregard it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prep_data = ml_client.components.create_or_update(prep_data)\r\n",
        "train_XGBoost = ml_client.components.create_or_update(train_XGBoost)"
      ],
      "outputs": [],
      "execution_count": 72,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685046643662
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(prep_data)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 75,
          "data": {
            "text/plain": "azure.ai.ml.entities._component.command_component.CommandComponent"
          },
          "metadata": {}
        }
      ],
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685046685372
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_id = '49c1ae66-5390-4431-930d-d51e183694af'"
      ],
      "outputs": [],
      "execution_count": 77,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685048409030
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model\r\n",
        "from azure.ai.ml.constants import ModelType\r\n",
        "\r\n",
        "\r\n",
        "job_name = \"49c1ae66-5390-4431-930d-d51e183694af\"\r\n",
        "\r\n",
        "run_model = Model(\r\n",
        "    path=f\"azureml://jobs/{job_name}/outputs/artifacts/paths/model/\",\r\n",
        "    name=\"churn_model\",\r\n",
        "    description=\"Model created from run.\",\r\n",
        "    type=AssetTypes.MLFLOW_MODEL,\r\n",
        ")\r\n",
        "# Uncomment after adding required details above\r\n",
        "ml_client.models.create_or_update(run_model)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 87,
          "data": {
            "text/plain": "Model({'job_name': '49c1ae66-5390-4431-930d-d51e183694af', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'churn_model', 'description': 'Model created from run.', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourceGroups/rg-churn-pred-proj/providers/Microsoft.MachineLearningServices/workspaces/churn-pred-proj/models/churn_model/versions/1', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cia6d263a341e24a4b96/code/Users/Ramin_Vali/Customer_Churn', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f7835161930>, 'serialize': <msrest.serialization.Serializer object at 0x7f7835161b70>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourceGroups/rg-churn-pred-proj/workspaces/churn-pred-proj/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.49c1ae66-5390-4431-930d-d51e183694af/model', 'datastore': None, 'utc_time_created': None, 'flavors': {'python_function': {'data': 'model.xgb', 'env': 'conda.yaml', 'loader_module': 'mlflow.xgboost', 'python_version': '3.8.16'}, 'xgboost': {'code': '', 'data': 'model.xgb', 'model_class': 'xgboost.sklearn.XGBClassifier', 'xgb_version': '1.7.5'}}, 'arm_type': 'model_version', 'type': 'mlflow_model'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685049914953
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_example = ml_client.models.get(name=\"run-model-example\", version=\"1\")\r\n",
        "print(model_example)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "creation_context:\n  created_at: '2023-05-25T21:22:21.940414+00:00'\n  created_by: Ramin Vali\n  created_by_type: User\n  last_modified_at: '2023-05-25T21:22:21.940414+00:00'\n  last_modified_by: Ramin Vali\n  last_modified_by_type: User\ndescription: Model created from run.\nflavors:\n  python_function:\n    data: model.xgb\n    env: conda.yaml\n    loader_module: mlflow.xgboost\n    python_version: 3.8.16\n  xgboost:\n    code: ''\n    data: model.xgb\n    model_class: xgboost.sklearn.XGBClassifier\n    xgb_version: 1.7.5\nid: azureml:/subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourceGroups/rg-churn-pred-proj/providers/Microsoft.MachineLearningServices/workspaces/churn-pred-proj/models/run-model-example/versions/1\njob_name: 49c1ae66-5390-4431-930d-d51e183694af\nname: run-model-example\npath: azureml://subscriptions/733ee2ff-5a9c-42a2-8242-b8a500eb9f53/resourceGroups/rg-churn-pred-proj/workspaces/churn-pred-proj/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.49c1ae66-5390-4431-930d-d51e183694af/model\nproperties: {}\ntags: {}\ntype: mlflow_model\nversion: '1'\n\n"
        }
      ],
      "execution_count": 86,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1685049851605
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}